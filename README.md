# üìä ETL Pipeline con Pandas y SQLAlchemy

Este proyecto es un primer acercamiento al procesamiento de datos con Python, enfocado en la pr√°ctica de t√©cnicas de ETL (Extract, Transform, Load).

## üöÄ Descripci√≥n

El proyecto se compone de tres partes principales:  

1Ô∏è‚É£ **Configuraci√≥n base de datos**  
Definici√≥n y creaci√≥n de las tablas de la base de datos    

2Ô∏è‚É£ **Proceso ETL**  
Parte principal del proyecto cuyo flujo de trabajo es el siguiente:
- Lectura de datos de archivos CSV
- Limpieza y transformaci√≥n de los datos. Eliminaci√≥n de valores nulos, renombre de columnas, transformaci√≥n de ciertos valores, creaci√≥n de nuevo atributo, etc.   
- Carga de datos limpios en la base de datos PostgreSQL
  
3Ô∏è‚É£ **Ejecuci√≥n de consultas**  
Analizar y extraer informaci√≥n relevante. Se incluyen algunas queries como ejemplo.

## üìÇ Sobre los datos  
Los datos utilizados en este proyecto provienen de este [dataset](https://www.kaggle.com/datasets/bhavikjikadara/mental-health-dataset/data).
Est√° compuesto por respuestas de personas encuestadas e incluye una variedad de atributos personales y psicol√≥gicos (ocupaci√≥n, d√≠as al a√±o en casa, cambios de humor, estr√©s creciente ...).  
Esta informaci√≥n permite explorar c√≥mo distintos factores personales y psicol√≥gicos se relacionan entre s√≠, y puede ser √∫til para analizar tem√°ticas relacionadas con la salud mental.

## üõ†Ô∏è Tecnolog√≠as usadas  
- Python üêç
- PostgreSQL üóÑÔ∏è
- Pandas.
- Prefect.
- SQLAlchemy.

## ‚öôÔ∏è Instalaci√≥n 
*Desarrollado en entorno Windows utilizando Visual Studio 
1. Descargar [PostgreSQL](https://www.postgresql.org/download/windows/). Una vez instalado, se puede usar pgAdmin para gestionar la base de datos.
   
2. Clonar el repositorio.  
   `git clone https://github.com/alvaroggomez/etl_pipeline.git`
   
3. Crear entorno virtual. Recomendable para manejar las dependencias del proyecto sin interferir con otras configuraciones del sistema.  
   `python3 -m venv env`
   
4. Instalar las dependencias.  
   `pip install -r requirements.txt`
   
5. Crear archivo .env en la ra√≠z del proyecto basado en el archivo .env.example. Recomendable seguir el ejemplo y crear una carpeta para los csvs a leer y otra donde se guardar√°n los csvs ya leidos y tranformados.  
    
6. Comprobar que se establece una conexi√≥n con la base de datos.  
   `python .py`  

7. Crear tablas necesarias en la base de datos. Para ello es necesario ejecutar el archivo *create_tables.py*  
   `python create_tables.py` 
    
Una vez se ha configurado todo el proyecto, se podr√° ejecutar el flujo ETL   
    `python main.py`

Si se desea hacer consultas se puede iniciar el archivo *queries.py*   
    `python queries.py`


## üìù Archivos
- **main.py**: Contiene el flujo principal de procesamiento de datos (ETL).
- **create_tables.py**: Creaci√≥n de las tablas de la base de datos.
- **data_processing.py**: Lee, limpia y transforma los datos de los archivos CSV.
- **load_data.py**: Carga los datos tranformados en la base de datos.
- **archivo_hash.py**: Funciones para comprobar si un archivo ya se ha procesado previamente. En caso negativo, le asigna un hash.
- **queries.py**: Consultas para analizar los datos.
- **conexion_db.py**: Archivo para comprobar que se ha establecido la conexion con la base de datos.
- **.env.example**: Variables necesarias como las credenciales de la base de datos y las rutas de los archivos.
- **requirements.txt**: Lista de dependencias del proyecto.
- **MentalHealthDataset.csv**: Dataset usado en este proyecto 






